{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "#from spacy.en.word_sets import STOP_WORDS\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import os\n",
    "#from spacy.en.word_sets import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load french tokenizer, tagger, parser, NER and word vectors\n",
    "#nlp = spacy.load('en')\n",
    "nlp = English() #Charge des règles de tokenisation\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process whole documents\n",
    "#text = (u\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        #u\"Google in 2007, few people outside of the company took him \"\n",
    "        #u\"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        #u\"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        #u\"worth talking to,” said Thrun, now the co-founder and CEO of \"\n",
    "        #u\"online higher education startup Udacity, in an interview with \"\n",
    "        #u\"Recode earlier this week.\")\n",
    "#doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "#for entity in doc.ents:\n",
    "    #print(entity.text, entity.label_)\n",
    "\n",
    "# Determine semantic similarities\n",
    "#doc1 = nlp(u\"my fries were super gross\")\n",
    "#doc2 = nlp(u\"such disgusting fries\")\n",
    "#similarity = doc1.similarity(doc2)\n",
    "#print(doc1.text, doc2.text, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for nom_fichier in os.listdir(\"cnn/stories\"):\n",
    "\n",
    "    if nom_fichier.endswith(\".story\"):\n",
    "        chemin = os.path.join(\"cnn/stories\",nom_fichier)\n",
    "        with open(chemin, \"r\") as fichier:\n",
    "            contenu = fichier.read()\n",
    "        articles.append(contenu)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer des paires de document (article, highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(articles[1])\n",
    "# split + tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corps,resume = articles[1][articles[1].find(\"@highlight\"):],articles[1][:articles[1].find(\"@highlight\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corps_resum(article):\n",
    "    ind_dep = article.find(\"@highlight\")\n",
    "    corps,resume = article[:ind_dep],article[ind_dep:]\n",
    "    return (corps,resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_corps_resum(articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupl_article = [split_corps_resum(article) for article in articles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@highlight\\n\\nCNN Radio anchor Stan Case dies in a car wreck\\n\\n@highlight\\n\\nHis wife, Angela Stiepel Case, was injured, a friend says\\n\\n@highlight\\n\\n\"Stan was a rock here,\" a colleague said'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tupl_article[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppression de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(string.punctuation)\n",
    "\n",
    "#sent = '''He said,\"that's it.\"'''\n",
    "#print(\" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in sent]).split()))\n",
    "\n",
    "\n",
    "tupl_article_without_puntuation = [(\" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in article]).split()),\n",
    " \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in resume]).split()))\n",
    " for article,resume in tupl_article]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation en token en minuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser ntlk, spacy ou faire une fonction\n",
    "tupl_doc = [(nlp(article.lower()),nlp(resume.lower())) for article,resume in tupl_article_without_puntuation]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tupl_doc[0][0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppression des stopwords pour les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword = mot qui ne contient pas d'info\n",
    "#tupl_doc_sans_stop = [ del tok_corps, del tok_resume  for l_tok_corps,l_tok_resume in tupl_doc if tok_corps.is_stop == True or tok_resume tok_resume.is_stop == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des fréquences et tf-idf sur les deux types de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire avec pour chaque mots sa fréquence ou objet python Counter() / tf (fréquence du mot dans un texte)-idf (inverse du nombre d'apparition du mot dans le corpus) = indicateur le plus utilisé dans l'analyse de text : Il permet de déterminer à quel point un mot est caractéristique d'un texte au sein d'un corpus de texte --> nltk ou spacy ou scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enregistrement du nouveau jeu de données d’entraînement pour usage ultérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
